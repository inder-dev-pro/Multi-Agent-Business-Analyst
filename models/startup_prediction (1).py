# -*- coding: utf-8 -*-
"""Startup_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PmzeIy9ern3GPTuGUUBEhJ9c_WQtKnoD
"""

import kagglehub
import pandas as pd
import plotly.figure_factory as ffdemo
import matplotlib.pyplot as plt
import seaborn as sea
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler
!pip install category_encoders
from category_encoders.target_encoder import TargetEncoder
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Flatten, Concatenate
# Download latest version
path = kagglehub.dataset_download("yanmaksi/big-startup-secsees-fail-dataset-from-crunchbase")

print("Path to dataset files:", path)
df=pd.read_csv(path+ r"/big_startup_secsees_dataset.csv")
df.head(5)
print(df.nunique())
print(df['status'].value_counts())
df.dropna(subset=['funding_total_usd'], inplace=True)

df['funding_total_usd'] = pd.to_numeric(df['funding_total_usd'], errors='coerce')
df['first_funding_at']=pd.to_datetime(df['first_funding_at'], errors='coerce')
df['last_funding_at']=pd.to_datetime(df['last_funding_at'], errors='coerce')
df['founded_at']=pd.to_datetime(df['founded_at'], errors='coerce')

"""

```
# Dividing the data into Train, Test and Val
```

"""

one_hot_encoding=OneHotEncoder()
one_hot_encoding.fit(df[['status']])
feature_names=one_hot_encoding.get_feature_names_out(['status'])
status_encoded=one_hot_encoding.transform(df[['status']]).toarray()
status_encoded_df=pd.DataFrame(status_encoded, columns=feature_names)
df=pd.concat([df,status_encoded_df], axis=1)

scaler=MinMaxScaler()
df[['funding_total_usd', 'funding_rounds']]=scaler.fit_transform(df[['funding_total_usd', 'funding_rounds']])

target_encoder=TargetEncoder()
df[['country_code']]=target_encoder.fit_transform(df['country_code'], df['status'])

vocab_size=27297
embedding_size=16
label_encoder=LabelEncoder()
embeddings=tf.keras.layers.Embedding(input_dim=vocab_size,output_dim=embedding_size)
df['category_index']=label_encoder.fit_transform(df['category_list'])
category_indices=tf.convert_to_tensor(df['category_index'], dtype=tf.int32)
embedded_values=embeddings(category_indices)
embedded_values=tf.keras.layers.Flatten()(embedded_values)
embedded_values.numpy()

train_val_data, test_data=train_test_split(df,test_size=0.2, random_state=42)
train_data, val_data=train_test_split(train_val_data, test_size=0.25, random_state=42)
input_cols = ['funding_total_usd', 'funding_rounds', 'first_funding_years', 'last_funding_years', 'country_code']
target_cols=[ 'status_acquired', 'status_closed', 'status_ipo', 'status_operating']

numerical_cols=df.select_dtypes(include=np.number).columns.to_list()
categorical_cols=df.select_dtypes(include=object).columns.to_list()

# Convert date columns to numerical format (Elapsed years from founding)
def convert_dates(df):
    df['first_funding_at'] = pd.to_datetime(df['first_funding_at'], errors='coerce')
    df['last_funding_at'] = pd.to_datetime(df['last_funding_at'], errors='coerce')
    df['founded_at'] = pd.to_datetime(df['founded_at'], errors='coerce')

    # Calculate elapsed years since founding
    df['first_funding_years'] = (df['first_funding_at'] - df['founded_at']).dt.days / 365.0
    df['last_funding_years'] = (df['last_funding_at'] - df['founded_at']).dt.days / 365.0

    # Fill NaNs with 0
    df[['first_funding_years', 'last_funding_years']] = df[['first_funding_years', 'last_funding_years']].fillna(0)

    return df

# Apply function
train_data = convert_dates(train_data)
val_data = convert_dates(val_data)
test_data = convert_dates(test_data)

plt.boxplot(x=df['funding_rounds'])
plt.yscale('log')
plt.show()

"""

```
Converting the status(Target col) into a Numeric Column
```

"""

outlier=[]
def outlier_detection(data):
  threshold=4
  mean=np.mean(data)
  std=np.std(data)

  for i in data:
    if ((i-mean)/std>3):
      outlier.append(i)

  return outlier
out=outlier_detection(df.funding_rounds)

outlier_df_z=pd.DataFrame(out, columns=["Funding_Round_Outlier"])
outlier_df_z

from matplotlib import pyplot as plt
outlier_df_z['Funding_Round_Outlier'].plot(kind='hist', bins=20, title='Funding_Round_Outlier')
plt.gca().spines[['top', 'right',]].set_visible(False)

iqr_outlier_list=[]
def IQR(data):
  q1,q3=np.percentile(data,[25,75])
  iqr=q3-q1
  lower_bound=q1-((iqr)*1.5)
  upper_bound=q3+((iqr)*1.5)
  for i in data:
    if (i<lower_bound or i>upper_bound):
      iqr_outlier_list.append(i)
  return iqr_outlier_list
iqr_out=IQR(df.funding_rounds)

outlier_df_iqr=pd.DataFrame(iqr_out,columns=["Outlier_using_IQR"])
outlier_df_iqr

"""Using Min Max Scaler for Numeric Cols to be used for model training

Target Encoding for Country Code

This is to convert high cardinality categorical data into numerical columns

Defining inputs and outputs for the keras model
"""

X_train = [train_data[input_cols].values.astype(np.float32), train_data['category_index'].values.reshape(-1, 1)]
y_train = train_data[target_cols].values.astype(np.float32)

X_val = [val_data[input_cols].values.astype(np.float32), val_data['category_index'].values.reshape(-1, 1)]
y_val = val_data[target_cols].values.astype(np.float32)

category_input = Input(shape=(1,), name='category_input')  # Categorical Feature
num_input = Input(shape=(5,), name='num_input')  # 5 Numerical Features (Fixing Shape)

"""Building a FeedForward Neural Network Model"""

merged = Concatenate()([category_input, num_input])
x = Dense(64, activation='relu')(merged)
x = Dense(32, activation='relu')(x)
x = Dense(16, activation='relu')(x)
output = Dense(4, activation='softmax', name="output")(x)
model = Model(inputs=[category_input, num_input], outputs=output)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

history = model.fit(
    X_train, y_train,
    epochs=10, batch_size=32, validation_data=(X_val, y_val)
)

test_loss, test_acc = model.evaluate(
    [test_data['category_index'].values, test_data[input_cols].values],
    test_data[['status_operating', 'status_closed', 'status_acquired', 'status_ipo']].values
)
print(f"Test Accuracy: {test_acc:.4f}")

print(df.first_funding_at)
df.nunique()
df.isnull().sum()
df.isna().sum()
df.info()
input_cols
numerical_cols

# Save the model
model.save("startup_prediction_model.h5")
print("Model saved successfully!")

model.predict()

new_data = {
    "funding_total_usd": [5000000],   # Example: $5M in funding
    "funding_rounds": [3],
    "last_funding_years": [5.0],
    "country_code": [0.75],
    "category_index": [15],
    "extra_feature": [1.0]  # Add an extra feature to match the expected input shape
}

# ✅ Convert Data to NumPy Arrays
X_numerical = np.array([new_data["funding_total_usd"],
                         new_data["funding_rounds"],
                         new_data["last_funding_years"],
                         new_data["country_code"],
                         new_data["extra_feature"]]).T.astype(np.float32)

X_category = np.array(new_data["category_index"]).reshape(-1, 1).astype(np.int32)

# Evaluate model to check if it's working
test_loss, test_acc = model.evaluate([X_numerical, X_category], np.array([[1, 0, 0, 0]]))
print(f"Test Loss: {test_loss}, Test Accuracy: {test_acc}")

# ✅ Make Prediction
predictions = model.predict([X_numerical, X_category])

# ✅ Convert to Class Label
status_classes = ["operating", "closed", "acquired", "ipo"]
predicted_index = np.argmax(predictions, axis=1)[0]

# ✅ Print Final Prediction
print("Predicted Startup Status:", status_classes[predicted_index])
print(model.summary())

!git init
!git remote add origin https://github.com/inder-dev-pro/EDA-Mark_Keith.git
!git branch -m main
!git add .
!git status
!git commit -m "Initial Upload"
!git config --global user.email "inder.31ssb.com"
!git config --global user.name "inder-dev-pro"
!git push origin main